// NOTE: Лекция 5. 01.09.2025
// HACK: vs code is cringe btw
#let nor = [независимых одинаково распределенных]
#let eqcircle = circle(height: 1em, stroke: 0.5pt, [#v(-0.3cm) #align(center + horizon, $eq$)])
= Выборка и её характер

// Всё, я тя вижу, давай лекцию слушать уже, я рот ебал в начале
// пары всё настраивать, а потом нихуя не понимать
// Понимаю

Пусть $xi$ --- наблюдаемая случайная величина.

Будем обозначать через $cal(L) (x, theta)$ --- закон распределения вероятностей 
- $x$ --- аргумент
- $theta$ --- параметр распределения

$ xi ~ cal(N) (a, sigma^2) #[--- нормальное распределение] $

если $xi ~ cal(N) (theta_1; theta_2)$, то парамметры $theta_1$ и $theta_2$ 
--- неизвестные соответствуют $a$ и $sigma^2$

Пусть за случайной величиной $xi$ произведено $n$ наблюдений.
Обозначим их как $X_1, X_2, dots, X_n$ --- случайные величины, распределённые 
так же, как $xi$. Будем считать, что эти величины независимые по условиям
эксперимента.

_Как обеспечивается независимость наблюдений на практике? Например, если
мы стоим на выходе у Пятёрочки и пытаемся узнать, сколько людей купило
апельсины, мы не спрашиваем только блондинок, а берём абсолютно рандомных
людей. Мы бы могли всех людей спрашивать, но это очень трудозатратно.
Здесь одним наблюдением $X_i$ будет вопрос одному человеку._

$xi_i$ --- независимые одинаково распределенные случайне величины

// Пиши заглавные иксы там, где говорится о наблюдениях
// чё ща происходит? Я вникнуть пытаюсь
// Как найти матожидание. А я тоже проебал это, увы
// Не, это я понял, а откуда у нас таблица появилась, чем являются столбцы
// и строки, я этого не понял
// бляха муха :(
$
  X_1, X_2, dots, X_n ~ xi (~ cal(L)_xi (x, theta))
$

*Вывод:* Основная задача математической статистики состоит в том, чтобы по
результатам наблюдений $X_1, X_2, dots, X_n$ сделать выводы о вероятностных
характеристиках величины $xi$.

/ Выборка из распределения случайной величины $xi$: --- Последовательность 
  $overline(X)_n = (x_1, dots, X_(n))$ случайных величин
  $~ cal(L)_xi (x, theta)$

/ Выборочное пространство: $XX_n = {overline(X_n) = (X_1, ..., X_n)}$.

/ Реализация выборки (выборка): --- Множество $(x_1, x_2, ..., x_n)$ 
  значений случайных величин $(X_1, X_2, ..., X_n)$

/ Генеральная совокупность значений $xi$: множество всех возможных значений.

/ Выборочная совпокупность: $(x_1, x_2, ..., x_n)$

*Пример:* \
Пусть $xi$ --- рост студента СГУ. \
Тогда все студенты СГУ --- это генеральная совокупность. \
Количество студентов $underbrace(N, #[объем генеральной\ совокупности]) = 30583$ \
Выборка $underbrace((175, 160, ..., 180), 1200 #[--- объём выборки])$ \
$overline(r)$ --- выборочное среднее. \
$overline(R)$ --- генеральное среднее (средний рост студента). \
// как приблизительное равно делать?
Очевидно, что $overline(r) != overline(R)$, но наша цель, чтобы 
$overline(r) approx overline(R)$ 

_Вот он бежит мимо школы. И случайно мальчика какого-то захватывает. И что 
мы получаем? Ну, метр двадцать_. То есть нужно следить за тем, чтобы в выборке 
не было элементов из другой генеральной совокупности.

// Там рост от 1.75 до 180)))

Для того, чтобы статистические выводы были надёжными, выборка должна быть
*репрезентативной*, то есть отражать свойства генеральной совокупности.

Рассмотрим выборку $(x_1, x_2, ..., x_n)$. 
/ Вариационным рядом называется : --- последовательность $x_i$, расположенная 
  в неубывающем порядке.
// HACK: Какая совокупность у тебя... Генеральская чтоль?

$
x_1^\* lt.eq x_2^\* lt.eq dots lt.eq x_n^\*;\
x_1^\* = min {x_i dots x_n}; quad
x_n^\* = max {x_i dots x_n}
$

/ Интервальный вариационный ряд: таблица вида

  #table(
    columns: (auto, auto, auto, auto, auto),
    [$(x_i ; x_(i+1)$], [$[x_1 ; x_2]$], [$(x_2 ; x_3]$], $dots$, [$(x_m ; x_(m + 1)]$],
    [$n_i$], [$n_1$], [$n_2$], $dots$, [$n_m$]
  )

  - $n_i$ --- частота --- количество наблюдений в интервале $(x_i, x_(i+1)]$ 
  - $x_i$ --- варианта
  - $m$ --- количество групп (интервал в данном случае)
  - $n = limits(sum)_(i=1)^m n_i$ --- объём выборки

_Примечание: оптимальное количество интервалов выбирается по формуле Стёджесса:_
// Тут не ceil, тут просто квадратная
$
  m = [1 + 3.321 lg n] = [1 + log_2 n]
$

/ Гистограмма частот: --- фигура:
// FIX: #image("") 1 рисунок

Гистограмма частот позволяет оценить вид функции плотности случайной величины $xi$

/ Точечный вариацонный ряд: --- таблица вида
  #table(
    columns: (auto, auto, auto, auto, auto),
    $x_i$, $x_1$, $x_2$, $dots$, $x_m$,
    $n_i$, $n_1$, $n_2$, $dots$, $n_m$
  )

  - $x_i$ --- варианты
  - $n_i$ --- частоты
  - $m$ --- количество групп
  - $n = limits(sum)_(i = 1)^m n_i$ --- объём выборки

Полигон частот (ломанная):
// FIX: #image("") 2 рисунок

/ Эмпирическая (опытная, выборочная) функция распределения (ЭФР):
// волна над F_n
  $
    limits(F_n)^~ (x) = sum_(i=1)^n e (x - X_i) #[либо] limits(F_n)^~ (x) mu_n (x) / n
  $
  - $mu_n (x)$ --- количетсво элементов выборки $< x$.
#let efr = "эмпирическая функция распределения"
*Свойства:*
+ $forall x in RR quad limits(F_n)^~ in [0, 1]$
+ $limits(F_n)^~ (x)$ --- ступенчатая кусочно непрерывная функция, неубывающая, 
  непрерывная слева
+ $#[При] x < x_1 quad limits(F_n)^~ (x) = 0$ \
  $#[При] x > x_n quad limits(F_n)^~ (x) = 1$
+ Случайная величина $mu_n (x) ~ B i n (n; F_xi (x))$ \
  Действительно при фиксированном $x$ рассмотрим событие ${X_i < x}$. Так как
  $X_i$ распределены, как $xi$, то $forall x space P{X_i < x} = P{xi < x}
  = F_xi (x)$\
  $cal(M) mu_n (x) = n dot p = n dot F_xi (x)$ \
  $cal(D) mu_n (x) = n p q = n F_xi (x) (1 - F_xi (x))$
+ $cal(M) limits(F_n)^~ (x) = F_xi (x)$ \
  $cal(D) limits(F_n)^~ (x) = (F_xi (x) (1 - F_xi (x))) / n$
+ *Теорема Гливенко* $limits(F_n)^~ (x) ->^p F_xi (x)$ \
  _Доказательство._ Действительно по неравенству Чебышёва

  $
    forall epsilon > 0 space P{|limits(F_n)^~ (x) - F_xi (x)| < epsilon}
    >= 1 - (overbrace(F_xi (x), in [0, 1]) overbrace((1 - F_xi (x)), 
    in [0, 1])) / (n epsilon^2) ->_(n -> infinity) 1
  $

  так как любая вероятность $<= 1$, то $exists limits(lim)_(n -> infinity) 
  P {|limits(F_n)^~ (x) - F_xi (x)| < epsilon} = 1 => 
  limits(F_n)^~ (x) ->^p F_xi (x)$













